{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"BERT Training","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"metadata":{"id":"OOB1-xSQF3KA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -qq transformers"],"metadata":{"id":"gdmZW2gnxRvO","executionInfo":{"status":"ok","timestamp":1651249268862,"user_tz":240,"elapsed":19044,"user":{"displayName":"Richard Tran","userId":"03618949930985817060"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"58189980-7d3f-4ea9-abb0-66e9bba4cbaa"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.0 MB 5.6 MB/s \n","\u001b[K     |████████████████████████████████| 77 kB 3.4 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 40.2 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 42.5 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 34.7 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch import optim, nn\n","import numpy as np\n","import transformers\n","from transformers import BertModel, BertTokenizer, RobertaModel, RobertaTokenizer, AdamW, get_linear_schedule_with_warmup\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"6Qw0A7WMxK9o","executionInfo":{"status":"ok","timestamp":1651249278698,"user_tz":240,"elapsed":9861,"user":{"displayName":"Richard Tran","userId":"03618949930985817060"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!pip install cloud-tpu-client==0.10 torch==1.11.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.11-cp37-cp37m-linux_x86_64.whl"],"metadata":{"id":"M8dhOPaCVUQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# imports the torch_xla package\n","import torch_xla\n","import torch_xla.core.xla_model as xm"],"metadata":{"id":"iz2gW8_xUuTp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = xm.xla_device() #torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"id":"cwy4clhAxoMi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def change_target(target):\n","    if target == -1:\n","        return 0\n","    \n","    else:\n","        return 1"],"metadata":{"id":"_xaOhLN9mpzY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.read_csv('/content/gdrive/Shareddrives/CS 5804: Artificial Intelligence/train.csv', index_col=False)\n","val = pd.read_csv('/content/gdrive/Shareddrives/CS 5804: Artificial Intelligence/val.csv', index_col=False)\n","test = pd.read_csv('/content/gdrive/Shareddrives/CS 5804: Artificial Intelligence/test.csv', index_col=False)\n","\n","train['target'] = train['target'].apply(change_target)\n","val['target'] = val['target'].apply(change_target)\n","test['target'] = test['target'].apply(change_target)"],"metadata":{"id":"KJaYqvBvwRLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","train, test = train_test_split(train.head(250000), test_size=0.2, random_state=24)\n","val, test = train_test_split(test, test_size=0.5, random_state=24)"],"metadata":{"id":"t6gHb9jlnTW4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Selection\n"],"metadata":{"id":"6IJagIp3xaW9"}},{"cell_type":"code","source":["model = \"bert-base-cased\"\n","tokenizer = BertTokenizer.from_pretrained(model)\n","model = BertModel.from_pretrained(model)"],"metadata":{"id":"daMZht1LxfL2","executionInfo":{"status":"ok","timestamp":1650430916230,"user_tz":240,"elapsed":4618,"user":{"displayName":"Richard Tran","userId":"03618949930985817060"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"79242135-aca0-4a74-c22a-13b88494eb3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"markdown","source":["## Dataset Preprocessing"],"metadata":{"id":"4TyrOx02xgD6"}},{"cell_type":"code","source":["class Sentiment140(Dataset):\n","  def __init__(self, texts, targets, tokenizer, max_length):\n","    self.texts = texts\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_length = max_length\n","    print(self.max_length)\n","  # Create these 2 methods in order to use Python's list indexer operator\n","  def __getitem__(self, curr_item):\n","    text = str(self.texts[curr_item])\n","    target = self.targets[curr_item] \n","    encode = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.max_length,  pad_to_max_length=True, truncation=True, return_token_type_ids=False, return_attention_mask=True, return_tensors='pt')\n","\n","    return {'tweets': text, 'input_ids': encode['input_ids'].flatten(), 'attention_mask': encode['attention_mask'].flatten(), 'targets': torch.tensor(target, dtype=torch.long)}\n","\n","  def __len__(self):\n","    return len(self.texts)"],"metadata":{"id":"RmSBbPNUxDrx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_data(df, batch_sz, max_length, tokenizer):\n","    sent_data = Sentiment140(texts=df.text.to_numpy(), targets=df.target.to_numpy(), tokenizer=tokenizer, max_length=max_length)\n","    return DataLoader(sent_data, batch_size=batch_sz, num_workers=2)"],"metadata":{"id":"WpSNwP3FxmiI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_sz = 32\n","max_length = 80\n","train_dataloader = load_data(train, batch_sz, max_length, tokenizer)\n","val_dataloader = load_data(val, batch_sz, max_length, tokenizer)\n","test_dataloader = load_data(test, batch_sz, max_length, tokenizer)\n","iter_train = next(iter(train_dataloader))\n","print(iter_train)"],"metadata":{"id":"YGM8utaVxmo1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dropout for regularization and fully-connected layer for output\n","class Transformer(nn.Module):\n","    def __init__(self, num_classes):\n","        super(Transformer, self).__init__()\n","        self.model = model\n","        self.dropout = nn.Dropout(p=0.3)\n","\n","        # get hidden units for each token and set to num_classes\n","        self.output = nn.Linear(self.model.config.hidden_size, num_classes)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        results = self.model(input_ids=input_ids, attention_mask=attention_mask)\n","        result = self.dropout(results.pooler_output)\n","        return self.output(result)\n"],"metadata":{"id":"DHHxwjmP0MDL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Transformer(num_classes=2)\n","model = model.to(device)\n","attention_mask = iter_train['attention_mask'].to(device)\n","input_ids = iter_train['input_ids'].to(device)\n","\n","# make sure the shape is batch size and max length\n","assert(attention_mask.shape[0] == batch_sz and attention_mask.shape[1] == max_length)\n","assert(input_ids.shape[0] == batch_sz and input_ids.shape[1] == max_length)\n","print(len(train_dataloader))"],"metadata":{"id":"_DxQvI6k0OX3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# params to try to replicate BERT paper \n","optim = AdamW(model.parameters(), lr=2e-5)\n","epochs = 10\n","num_steps = len(train_dataloader) * epochs\n","\n","loss_function = nn.CrossEntropyLoss().to(device)\n","\n","scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps=num_steps)"],"metadata":{"id":"AtEGrfPpkAXa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Training\n"],"metadata":{"id":"DpuUPcG1kNHP"}},{"cell_type":"code","source":["def training(model, data, optimizer, scheduler):\n","    model = model.train()\n","    correct = 0.0\n","    total_loss = []\n","    count = 0\n","    for curr in data:\n","\n","        labels = curr['targets'].to(device)\n","        input_ids = curr['input_ids'].to(device)\n","        attention_mask = curr['attention_mask'].to(device)\n","\n","        output = model(input_ids=input_ids, attention_mask=attention_mask)\n","        \n","        #get argmax\n","        _, arg_pred = torch.max(output, dim=1)\n","        loss = loss_function(output, labels)\n","        correct += torch.sum(arg_pred == labels)\n","        total_loss.append(loss.item())\n","        loss.backward()\n","\n","        # employ gradient clipping to avoid exploding gradients\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","        optim.step()\n","        scheduler.step()\n","        optim.zero_grad()\n","\n","        if count % 1000 == 0:\n","            print(count)\n","        count += 1\n","        print(count)\n","\n","\n","    return np.mean(total_loss), correct / len(data)      "],"metadata":{"id":"Y1WZ01SVkQDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def validation(model, data, optimizer, scheduler):\n","    model = model.eval()\n","    correct = 0.0\n","    total_loss = []\n","\n","    with torch.no_grad():\n","\n","        for curr in data:\n","            attention_mask = curr['attention_mask'].to(device)\n","            labels = curr['targets'].to(device)\n","            input_ids = curr['input_ids'].to(device)\n","            output = model(input_ids=input_ids, attention_mask=attention_mask)\n","            value_pred, arg_pred = torch.max(output, dim=1)\n","            correct += torch.sum(arg_pred == labels)\n","            loss = loss_function(output, labels)\n","            total_loss.append(loss.item())\n","\n","    return np.mean(total_loss), correct / len(data)"],"metadata":{"id":"zJ4kNBQDkX1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import trange"],"metadata":{"id":"iuTmE-odygv3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_acc = 0.0\n","\n","for epoch in trange(epochs):\n","    print('Epoch: {},'.format(epoch+1))\n","\n","    training_loss, training_acc = training(model, train_dataloader, optim, scheduler)\n","\n","    print('Training Loss: {}, Training Accuracy: {}'.format(training_loss, training_acc))\n","\n","    val_loss, val_acc = validation(model, val_dataloader, optim, scheduler)\n","\n","    print('Validation Loss: {}, Validation Accuracy: {}'.format(val_loss, val_acc), '\\n')\n","\n","    if val_acc > max_acc:\n","        torch.save(model.state_dict(), 'SentimentAnalysis.bin')\n","        max_acc = val_acc"],"metadata":{"id":"w0FQwAcPkxBN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib\n","%matplotlib\n","!python autograder.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZCN_NbSHmN0","executionInfo":{"status":"ok","timestamp":1651255286869,"user_tz":240,"elapsed":4917176,"user":{"displayName":"Richard Tran","userId":"03618949930985817060"}},"outputId":"b1b4c0f3-c545-45cb-ecfe-4b64dd69c3b5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Using matplotlib backend: agg\n","\n","Question q1\n","===========\n","*** q1) check_perceptron\n","Sanity checking perceptron...\n","autograder.py:342: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n","  expected_prediction = np.asscalar(np.where(np.dot(point, p.get_weights().data.T) >= 0, 1, -1))\n","Sanity checking perceptron weight updates...\n","Sanity checking complete. Now training perceptron\n","<Figure size 640x480 with 1 Axes>\n","*** PASS: check_perceptron\n","\n","### Question q1: 8/8 ###\n","\n","Question q2\n","===========\n","*** q2) check_regression\n","<Figure size 640x480 with 1 Axes>\n","Your final loss is: 0.001795\n","*** PASS: check_regression\n","\n","### Question q2: 8/8 ###\n","\n","Question q3\n","===========\n","*** q3) check_digit_classification\n","<Figure size 640x480 with 10 Axes>\n","\n","\n","Caught KeyboardInterrupt: aborting autograder\n","\n","Finished at 18:01:29\n","\n","Provisional grades\n","==================\n","Question q1: 8/8\n","Question q2: 8/8\n","Question q3: 3/9\n","------------------\n","Total: 19/25\n","\n","Your grades are NOT yet registered.  To register your grades, make sure\n","to follow your instructor's guidelines to receive credit on your project.\n","\n","\n","[autograder was interrupted before finishing]\n"]}]}]}