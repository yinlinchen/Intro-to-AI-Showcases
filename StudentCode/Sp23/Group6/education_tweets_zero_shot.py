# -*- coding: utf-8 -*-
"""Education tweets - zero shot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ON93te5qhEwpczM9Dt6uTH_JFA-mcPJE

Question "start"

## Setup
"""

item = "tweets"

topic = "tweets"

run_date = "20230422"

from google.colab import drive
drive.mount('/content/drive')

"""###Installations

Attempt to load text from folder shared with me
"""

# Mount data from drive
from google.colab import drive
drive.mount('/content/drive')

import os
import sys
# sys.path.append("/content/drive/Shareddrives/Working Group - NLP in Engineering Education Research/Fall 2021 Independent Study/Sample code")

# import sys
# sys.path.append("/content/drive/MyDrive/AK Faculty/Research/personal_utilities")

# !pip install -U spacy

# !pip install umap-learn
# !pip install hdbscan
# !pip install -U sentence-transformers

!pip install transformers

# !pip install keybert

!git clone https://github.com/andrewskatz/test_utilities.git

sys.path.append("/content/test_utilities/")
# sys.path.append("/content/drive/MyDrive/AK Faculty/Research/personal_utilities")

"""### Package Imports"""

import pandas as pd
import seaborn as sns
import matplotlib as plt
import pickle
import math
import numpy as np

import zs_labeling as zsl

# import label_cluster as lc

# sys.path.append("/content/drive/Shareddrives/Project - Metacognition and SRL Self-reflections/EF 152 Exam Wrapper Data/data analysis - yasir/All exam wrapper/")

"""### Data Imports"""

data_dir = "/content/drive/Shareddrives/AI Squad/Education Tweets" #change working
analysis_dir = working_dir = "/content/drive/Shareddrives/AI Squad/Education Tweets" #change analysis dir
os.chdir(data_dir)
os.listdir()

# =============================================================================
# # create unlabeled_df
# =============================================================================

text_sentence_df = pd.read_csv("random_tweets_sample_200_20230422.csv")
print(text_sentence_df.columns)

# add id

text_sentence_df['new_sent_id'] = text_sentence_df.index

unlabeled_df = text_sentence_df

unlabeled_df

# =============================================================================
# #create example_df
# =============================================================================
# os.chdir("./Analysis - os_myid")
# os.listdir()

# labeled_df = pd.read_csv( 'Analysis - os_lexrank_summary_df_20221102 - labeled_clusters.csv')
# labeled_df['sent_id'] = labeled_df.index

# example_df = labeled_df[labeled_df['homogenous'] == 1]

# # example_df = labeled_df

# # need to add sent_id to example_df
# # sent_plus_id = unlabeled_df[['sent_id', 'split_sent']]

# # example_df = pd.merge(example_df, sent_plus_id, on = 'split_sent', how = 'left')


# #example_df = example_df.drop_duplicates(subset=['sent_id', 'split_sent', 'example_label'])
# example_df = example_df.drop_duplicates(subset=['sent_id'])

os.chdir(analysis_dir)
os.listdir()

### Option using Google Sheet


from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

label_worksheet = gc.open('clean_text_mpnet__pca_80_agg_threshold__15_n_comp_5_n_neig_2_20230414_us').worksheet("Codebook") # should get changed to an unaugmented version

# get_all_values gives a list of rows.
rows = label_worksheet.get_all_values()
print(rows)

# Convert to a DataFrame and render.

labels_df = pd.DataFrame(rows[1:], columns=rows[0])



# example_df = labeled_df[labeled_df['remove_flag'] != 1]
# example_df.columns

labels_df

# =============================================================================
# Prepare for labeling
# =============================================================================




# pickle_in = open("ethics_dep_req_all-miniLM_sent_20220510.pickle", 'rb')
# full_embeddings = pickle.load(pickle_in)
# pickle_in.close()

#unlabeled_df.drop_duplicates('sent_id')
#group_size = 50
#unlabeled_df = unlabeled_df.reset_index(drop=True)

# unlabeled_df.apply(lambda row: math.floor(row.index / group_size), axis = 1)
#unlabeled_df['new_index'] = unlabeled_df.index
#unlabeled_df['batch_num'] = np.floor(unlabeled_df['new_index'] / group_size)

#unlabeled_df

# make sure column names are noted
print(unlabeled_df.columns)

"""# Labeling"""

from transformers import pipeline
classifier = pipeline("zero-shot-classification",
                      model="facebook/bart-large-mnli")

class_labels = list(labels_df['Topic'].unique())
print(len(class_labels))

class_labels

# classifier(test_text, class_labels)

# classifier_results = classifier(test_text, class_labels)

# pd.DataFrame(classifier_results)

# results = pd.DataFrame(classifier_results)
# zs_threshold = 0.15
# results[results['scores'] > zs_threshold]

"""##### Test labeling"""

filtered_df = unlabeled_df.dropna(subset=['clean_text'])

test_size = 200

test_df = filtered_df.sample(n = test_size, random_state=123) #use filtered_df. Do filtering

test_df #dropna

zs_threshold = 0.12

"""##### New Method using zsl and github clone"""

zs_threshold = 0.12

text_col_name = 'clean_text'
id_col_name = 'new_sent_id'
multi_label = False
keep_top_n = True
top_n = 5

total_results_df = zsl.label_df_with_zs(test_df, 
                                        text_col_name, 
                                        id_col_name, 
                                        class_labels, 
                                        zs_threshold, 
                                        multi_label=multi_label,
                                        keep_top_n=keep_top_n,
                                        top_n=top_n)

zs_threshold_save = str(zs_threshold).replace('.', '-')

if multi_label == True:
    total_results_df.to_csv(f"{topic}_zs_label_{zs_threshold_save}thresh_multi_{run_date}.csv", index = False)

if multi_label == False:
    total_results_df.to_csv(f"{topic}_zs_label_{zs_threshold_save}thresh_no-multi_{run_date}.csv", index = False)

"""##### Old method"""

# test_results_df = pd.DataFrame(columns=['sequence', 'labels', 'scores'])
# test_results_df

# for index, row in test_df.iterrows():
#   row_text = row['split_sent']
#   print(f"working on item {index}: {row_text}")
  
#   classifier_results = classifier(row_text, class_labels)
#   results_df = pd.DataFrame(classifier_results)
  
#   results_df = results_df[results_df['scores'] > zs_threshold]

#   test_results_df = pd.concat([test_results_df, results_df])

# test_results_df[test_results_df['scores'] > zs_threshold]

# test_results_df.to_csv(f"{topic}_{item}_test_labeled_zero_shot_{zs_threshold}thresh_bart_{run_date}.csv", index = False)

"""### Full Zero Shot Labeling"""



zs_threshold = 0.15

total_results_df = pd.DataFrame(columns=['sequence', 'labels', 'scores'])
total_results_df

for index, row in q_unlabeled_df.iterrows():
  row_text = row['split_sent']
  print(f"working on item {index}: {row_text}")
  
  classifier_results = classifier(row_text, class_labels)
  results_df = pd.DataFrame(classifier_results)
  
  results_df = results_df[results_df['scores'] > zs_threshold]

  total_results_df = pd.concat([total_results_df, results_df])

total_results_df[total_results_df['scores'] > zs_threshold]

# randomly sample and save for evaluation of top_score approach
# sim_ex_df.to_csv(f"{item}_all_labeled_top_score_20221115.csv", index = False)

# ran once on 20221119
if not use_aug_labeled:
  total_results_df.to_csv(f"{topic}_{item}_all_labeled_zero_shot_bart_{run_date}.csv", index = False)

if use_aug_labeled:
  total_results_df.to_csv(f"{topic}_{item}_aug_all_labeled_zero_shot_bart_{run_date}.csv", index = False)



"""# Extra block for keeping nb alive"""

keep_alive = 1

keep_alive = 2

keep_alive = 3

keep_alive = 4

keep_alive = 5

keep_alive = 6

keep_alive = 7