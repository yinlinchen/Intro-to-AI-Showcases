# -*- coding: utf-8 -*-
"""Education_tweets_clustering_20230414.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ybgpzTPUVS46kbOYevvWND4Z1mnKdTUu

Education CHATGPT Tweets

## Setup
"""

from google.colab import drive
drive.mount('/content/drive')

"""###Installations

Attempt to load text from folder shared with me
"""

!pip install rpy2==3.5.1

# Commented out IPython magic to ensure Python compatibility.

# %load_ext rpy2.ipython

# Mount data from drive
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%R
# system('sudo apt-get install -y libpoppler-cpp-dev', intern=TRUE)

# Commented out IPython magic to ensure Python compatibility.
# %%R
# install.packages("readtext")

!git clone https://github.com/crabcamp/lexrank.git

# needed for lexrank
!pip install urlextract

import sys
sys.path.append('/content/lexrank/')

from lexrank.lexrank import degree_centrality_scores

!pip install umap-learn
!pip install hdbscan
!pip install -U sentence-transformers

!pip install --upgrade spacy

!python -m spacy download en_core_web_lg
# typical guidance is to restart runtime after downloading

"""### Package Imports"""

import os
import sys
sys.path.append("/content/drive/Shareddrives/Working Group - NLP in Engineering Education Research/Fall 2021 Independent Study/Sample code")

import embed_cluster as ec

import pickle
import matplotlib.pyplot as plt
import numpy as np
import os
import pandas as pd
import re
import seaborn as sns
import umap
#import umap.plot
#import hdbscan
import spacy
from spacy.lang.en import English

from sklearn.manifold import MDS, TSNE


import pickle

# Commented out IPython magic to ensure Python compatibility.
# %%R
# library(tidyverse)

"""### Data Imports"""

working_dir = "/content/drive/Shareddrives/AI Squad/Education Tweets" #change working
os.chdir(working_dir)
os.listdir()

response_df = pd.read_csv("unique_chatgpt_edu_feb23_cleaned.csv") #csv

response_df.columns

response_df.shape

response_df = response_df.drop_duplicates(subset=['text', 'clean_text'], keep="first", inplace=False)

response_df.shape

response_df

"""### Utility Functions"""

# =============================================================================
# Utility functions
# =============================================================================


def select_and_filter(original_df, text_column):
    """
    Parameters
    ----------
    item : str
        Name of the column to filter based on.
    original_df : dataframe
        Original input dataframe with text columns.

    Returns
    -------
    filtered_df : dataframe
        Dataframe that has removed the NAs in the {item} column.
    item_list : list
        List of the text in the {item} column. This list is passed to the transformer model.
    """
    original_df['added_id'] = original_df.index + 1
    filtered_df = original_df.loc[pd.notnull(original_df[text_column])]
    print(f"Filtered dataframe for {text_column}.")
    print(f"Dataframe has size {filtered_df.shape}.")
    
    item_list = filtered_df[text_column].to_list()
    print(f"The list has length {len(item_list)}.")
        
    return filtered_df, item_list

def sentence_segmenter(data_frame, text_column):
    # first, create list from series of text
    
    entry_list = data_frame[text_column].to_list()
    
    # try using dictionary of lists
    
    new_df_dict = {'text_column': [],
                   'original_id': [],
                   #'original_entry': [],
                   # drop the original entry since that is the full transcript response for a participant
                   'split_sent': [],
                   'sent_num': []}
    
    #nlp = English()
    nlp.add_pipe('sentencizer')
    
    ## using the process of converting series to list and then iterating over list
    for i, entry in enumerate(entry_list):
        doc = nlp(entry)
        sentences = [sent.text.strip() for sent in doc.sents]
        for j, sent in enumerate(sentences):
            new_df_dict['text_column'].append(text_column)
            new_df_dict['original_id'].append(id) 
            #new_df_dict['original_entry'].append(entry)
            #drop the original entry since that is the full transcript response for a participant
            new_df_dict['split_sent'].append(sent)
            new_df_dict['sent_num'].append(j)
            
    sentence_df = pd.DataFrame(new_df_dict)
    
    item_list = sentence_df['split_sent'].to_list()
    
    print(f"Filtered dataframe for {text_column}.")
    print(f"Dataframe has size {sentence_df.shape}.")
    print(f"The list has length {len(item_list)}.")

    
    return sentence_df, item_list

def sentence_segmenter(data_frame, text_column):
    nlp = spacy.load("en_core_web_lg")
    new_df_dict = {'split_sent': [], 'doc': [], 'sent_num': [], 'original_sent': []}
    
    for i, row in data_frame.iterrows():
        doc = nlp(row[text_column])
        for j, sent in enumerate(doc.sents):
            new_df_dict['split_sent'].append(str(sent).strip())
            new_df_dict['doc'].append(doc)
            new_df_dict['sent_num'].append(j)
            new_df_dict['original_sent'].append(str(sent).strip())
            
    sentence_df = pd.DataFrame(new_df_dict)
    item_list = sentence_df['split_sent'].to_list()
    
    return sentence_df, item_list

"""## Analysis

### Data pre-processing

#### Change these as needed
"""

notebook_topic = "clean_text" #column with clean text in csv

run_date = "20230414"

response_df.columns

response_filtered_df, response_filtered_list = select_and_filter(response_df, notebook_topic)
# 1089 x 14

response_filtered_df['clean_text']= response_filtered_df['clean_text'].astype(str)
response_filtered_df

response_filtered_df = response_filtered_df.drop_duplicates(subset=['clean_text'], keep="first", inplace=False)
response_filtered_df

tweet_list = response_filtered_df['clean_text'].tolist()
tweet_list





"""### Sentence Embedding"""

sent_embedding = ec.embed_raw_text(tweet_list, 'mpnet', max_seq_length=200)

# pickle the embeddings since there are 2467 sentences in the full 53k observation dataset
pickle_out = open(f'{notebook_topic}_sentence_mpnet_{run_date}_us.pickle', 'wb')
pickle.dump(sent_embedding, pickle_out)
pickle_out.close()

pickle_in = f'{notebook_topic}_sentence_mpnet_{run_date}_us.pickle'
PATH = "/content/drive/Shareddrives/AI Squad/Education Tweets" #save to my directory
path_to_embed = os.path.join(PATH, pickle_in)
sent_embedding = pickle.load(open(path_to_embed, "rb"))

"""### Sentence Dimension Reduction"""

embed_param_dict = {'pca_dim': 80,
                   'n_neighbors': 2,
                   'min_dist': 0.0,
                   'n_components': 5,
                   'metric': 'cosine',
                   'random_state': 123}

embed_param_title = "pca_dim:" + str(embed_param_dict['pca_dim']) + ', n_nei:' + str(embed_param_dict['n_neighbors'])

lower_embed = ec.project_original_embedding(sent_embedding, 
                                            embed_param_dict, 
                                            to_low = True, 
                                            mid_to_low_method='umap',
                                            title = embed_param_title)

"""### Clustering"""

cluster_param_dict = {'min_cluster_size': 5, # hdbscan options
                      'min_samples': 1,
                      'cluster_selection_epsilon': 0.1,
                      'alpha': 1.0,
                      'metric': 'euclidean',
                      'agg_type': "threshold", # agglomerative options - can be threshold or n_cluster
                      'n_clusters': 50,
                      'threshold_val': 15,
                      'affinity': 'euclidean',
                      'linkage': 'ward',
                      'num_clusters': 30}

# version to use the lower-dimensional embedding for the clustering     
cluster_res = ec.cluster_embedding(data=lower_embed, original_corpus_list=tweet_list, 
                  model='agglomerative', param_dict=cluster_param_dict, plot_option=True)

all_cluster_labels = cluster_res.labels_

response_filtered_df['cluster_label'] = all_cluster_labels
response_filtered_df

response_filtered_df.to_csv(f"{notebook_topic}_mpnet__pca_80_agg_threshold__15_n_comp_5_n_neig_2_{run_date}_us.csv")

"""### Cluster Summarization"""

response_sentence_df = pd.read_csv("clean_text_mpnet__pca_80_agg_threshold__15_n_comp_5_n_neig_2_20230414_us.csv")

import nltk
from sentence_transformers import SentenceTransformer, util
import numpy as np
from lexrank import LexRank
from lexrank.lexrank import degree_centrality_scores
import math

np.sort(response_sentence_df.cluster_label.unique())

response_sentence_df

response_sentence_df = response_sentence_df.rename(columns={'Unnamed: 0': 'sent_id'})

response_sentence_df = response_sentence_df



# trying different model for embedding: all-MiniLM-L12-v2 to see if there's a difference compared to mpnet


lexrank_summary_dict = {'sent_id': [],
                         'original_id': [],
                         'cluster': [],
                         'sum_sent': []}


bad_clusters = []



for i in range(response_sentence_df['cluster_label'].nunique()):
    try:
      print(f"\nWorking on cluster {i}.")
      temp_cl_df = response_sentence_df[response_sentence_df['cluster_label'] == i]
      
      # decided not to use previously calculated embeddings because they would need to be converted to tensor
      # test_embed = test_df.loc[:,'0':'74']
      
      
      sentences = temp_cl_df['clean_text'].to_list()
      
      model = SentenceTransformer('all-MiniLM-L12-v2')
      embeddings = model.encode(sentences, convert_to_tensor=True)
      # embeddings = test_embed
      
      print(f"Finished embedding for cluster {i}.")
      
      
      #Compute the pair-wise cosine similarities
      cos_scores = util.pytorch_cos_sim(embeddings, embeddings).numpy()
      
      print(f"Calculated cosine similarities for cluster {i}.")
      #Compute the centrality for each sentence
      centrality_scores = degree_centrality_scores(cos_scores, threshold=None)
      
      print(f"Completed centrality scores for cluster {i}.")
      #We argsort so that the first element is the sentence with the highest score
      most_central_sentence_indices = np.argsort(-centrality_scores)
      cl_sent_num = len(sentences)
      sum_sent_cap = int(math.ceil(cl_sent_num * 0.2))
      
      #Print the top 20% of sentences with the highest scores
      print(f"Summary for cluster {i}: ")
      for idx in most_central_sentence_indices[0:sum_sent_cap]:
          print(sentences[idx].strip())
          
          lexrank_summary_dict['sent_id'].append(temp_cl_df['sent_id'].iloc[idx])
          lexrank_summary_dict['original_id'].append(temp_cl_df['original_id'].iloc[idx])
          lexrank_summary_dict['cluster'].append(i)
          lexrank_summary_dict['sum_sent'].append(sentences[idx].strip())
    
    except:
      bad_clusters.append(i)

print(bad_clusters)

sent_id

lexrank_summary_df = pd.DataFrame(lexrank_summary_dict)

lexrank_summary_df

run_date = 20230417

lexrank_summary_df.to_csv(f"{notebook_topic}_lexrank_summary_df_{run_date}.csv", index = False)