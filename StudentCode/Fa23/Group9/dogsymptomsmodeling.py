# -*- coding: utf-8 -*-
"""DogSymptomsModeling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BFYDcJDg3eoTsU19HR14itK2LpfeLrG2
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import metrics
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
from sklearn.metrics import confusion_matrix, classification_report

from sklearn.model_selection import train_test_split

from sklearn import metrics
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
from sklearn.metrics import confusion_matrix, classification_report

df = pd.read_csv('pre_processed.csv') # load dataset

df.head()

data = df.drop(['Unnamed: 0', 'Disease'], axis=1) # drop unused columns

data.info()

X = data.drop(['Tick fever', 'Distemper', 'Parvovirus',
       'Hepatitis', 'Tetanus', 'Chronic kidney Disease', 'Diabetes',
       'Gastrointestinal Disease', 'Allergies', 'Gingitivis', 'Cancers',
       'Skin Rashes'], axis=1)

y = data[['Tick fever', 'Distemper', 'Parvovirus',
       'Hepatitis', 'Tetanus', 'Chronic kidney Disease', 'Diabetes',
       'Gastrointestinal Disease', 'Allergies', 'Gingitivis', 'Cancers',
       'Skin Rashes']]

X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=66)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=66)

# Show how many data in each set
print("Length of train set:", len(X_train))
print("Length of validation set:", len(X_val))
print("Length of test set:", len(X_test))

import torch

import torch.nn as nn
import torch.nn.functional as F

from torch.utils.data import Dataset, DataLoader

class DogSymptomsDataset(Dataset):
  def __init__(self, X, Y):
    self.X = torch.tensor(X.values, dtype=torch.float32)
    self.Y = torch.tensor(Y.values, dtype=torch.float32)

  def __len__(self):
    return len(self.X)

  def __getitem__(self, index):
    return (self.X[index], self.Y[index])

train_ds = DogSymptomsDataset(X_train, y_train)

val_ds = DogSymptomsDataset(X_val, y_val)

test_ds = DogSymptomsDataset(X_test, y_test)

trainloader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)
valloader = DataLoader(val_ds, batch_size=32, shuffle=True, num_workers=2)

testloader = DataLoader(test_ds, batch_size=32, shuffle=True, num_workers=2)

for i, (data, labels) in enumerate(trainloader):
  print(data.shape, labels.shape)
  print(data,labels)
  break;

class MLP(nn.Module):
  def __init__(self):
    super(MLP, self).__init__()
    self.layers = nn.Sequential(
        nn.Linear(86, 256),
        nn.Dropout(0.2),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.Dropout(0.2),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.Dropout(0.2),
        nn.ReLU(),
        nn.Linear(64, 12),
        # nn.Softmax()

    )
  def forward(self, x):
    return self.layers(x)

def train():
  mlp = MLP()
  torch.manual_seed(666)
  trainloader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=2)
  loss_function = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(mlp.parameters(), lr=2e-4)
  train_loss_list = []
  val_loss_list = []
  train_f1_list=[]
  val_f1_list=[]
  for epoch in range(10):
    # Print epoch
    print(f'Starting epoch {epoch+1}')
    loss_epoch_list = []
    pred_list = []
    label_list = []

    # Set current loss value

    current_loss = 0.0
    # Iterate over the DataLoader for training data
    for i, data in enumerate(trainloader):

      # Get inputs
      inputs, targets = data

      # Zero the gradients
      optimizer.zero_grad()

      # Perform forward pass
      outputs = mlp(inputs)
      pred_list.append(outputs)
      label_list.append(targets)

      # Compute loss
      loss = loss_function(outputs, targets)

      # Perform backward pass
      loss.backward()

      # Perform optimizationd
      optimizer.step()

      # Print statistics

      loss_item = loss.item()

      loss_epoch_list.append(loss_item)

      current_loss += loss_item
      if (i !=0) and (i % 50 == 0):

          train_loss_list.append(current_loss/50)
          train_f1 = f1([outputs], [targets])
          train_f1_list.append(train_f1)

          val_loss, val_f1 = validation(mlp)
          val_loss_list.append(val_loss)
          val_f1_list.append(val_f1)


          print('Loss after mini-batch %5d: %.3f' %
                (i, current_loss/50))
          current_loss = 0.0


    # print(sum(loss_epoch_list)/len(trainloader))
    # train_loss_list.append(sum(loss_epoch_list)/len(trainloader))
    # train_f1 = f1(pred_list, label_list)
    # print(train_f1)
    # train_f1_list.append(train_f1)



  # Process is complete.
  print('Training process has finished.')
  return (mlp, train_loss_list,val_loss_list,train_f1_list,val_f1_list)



def f1(pred_list, label_list):
  pred_all = torch.cat(pred_list)
  label_all = torch.cat(label_list)
  y_pred_classes = np.argmax(pred_all.detach().numpy(), axis=1)
  y_true_classes = np.argmax(label_all.detach().numpy(), axis=1)
  _f1 = f1_score(y_true_classes, y_pred_classes, average='weighted')*100
  return _f1

def validation(model):
  pred_list = []
  label_list = []
  loss_list = []
  loss_function = nn.CrossEntropyLoss()
  for i, data in enumerate(valloader):
      # Get inputs
      inputs, targets = data
      # Perform forward pass
      outputs = model(inputs)
      loss = loss_function(outputs, targets)
      loss_list.append(loss.item())
      pred_list.append(outputs)
      label_list.append(targets)

  avg_loss = sum(loss_list)/len(valloader)
  avg_f1 = f1(pred_list, label_list)
  return (avg_loss, avg_f1)

# print(classification_report(y_true=y_test_nump, y_pred=y_pred_ohe))

# print('F1-score% =', f1_score(y_test_nump, y_pred_ohe, average='weighted')*100, '|',
#       'Accuracy% =', accuracy_score(y_test_nump, y_pred_ohe)*100,
#       'Recall% =', recall_score(y_test_nump, y_pred_ohe, average='weighted')*100, '|',
#       'Precision% =', precision_score(y_test_nump, y_pred_ohe, average='weighted')*100)

model, train_loss_list,val_loss_list,train_f1_list,val_f1_list = train()

train_loss = train_loss_list
val_loss = val_loss_list
train_acc = train_f1_list
val_acc = val_f1_list

# Draw loss plot
plt.figure(figsize = (6,6))
plt.plot(train_loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.xlabel('Batch')
plt.ylabel('Loss')
plt.legend()
plt.title('Loss')
plt.show()


# Draw accuracy plot
plt.figure(figsize = (6,6))
plt.plot(train_acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel('Batch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Accuracy')
plt.show()

def test(model):
  pred_list = []
  label_list = []
  for i, data in enumerate(testloader):
      # Get inputs
      inputs, targets = data
      # Perform forward pass
      outputs = model(inputs)
      pred_list.append(outputs)
      label_list.append(targets)
  pred_all = torch.cat(pred_list)
  label_all = torch.cat(label_list)
  y_pred_classes = np.argmax(pred_all.detach().numpy(), axis=1)
  y_true_classes = np.argmax(label_all.detach().numpy(), axis=1)
  avg_f1 = f1(pred_list, label_list)
  return (avg_f1, y_pred_classes, y_true_classes)

avg_f1, y_pred_classes, y_true_classes = test(model)



dnames = df['Disease'].unique().tolist()

print(classification_report(y_true_classes, y_pred_classes,output_dict=False))

report = classification_report(y_true_classes, y_pred_classes,output_dict=True)

report2={}
for i in range(len(dnames)):
  report2[dnames[i]] = report[str(i)]
report2['macro avg'] = report['macro avg']
report2['weighted avg'] = report['weighted avg']

pd.DataFrame(report2).T

conf_matrix = metrics.confusion_matrix(y_true_classes, y_pred_classes)

df_cm = pd.DataFrame(conf_matrix, index=df['Disease'].unique(), columns=df['Disease'].unique())
print('F1-score% =', f1_score(y_true_classes, y_pred_classes, average='weighted')*100, '|',
      'Accuracy% =', accuracy_score(y_true_classes, y_pred_classes)*100, '|',
      'Recall% =', recall_score(y_true_classes, y_pred_classes, average='weighted')*100, '|',
      'Precision% =', precision_score(y_true_classes, y_pred_classes, average='weighted')*100
      )
plt.figure(figsize = (8,8))
sns.heatmap(df_cm)

fig, ax = plt.subplots(figsize=(10, 10))
ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')

plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix of ANN Model', fontsize=18)
plt.show()

def predict(model, item):
  p = model(item[0])
  y_pred_class = np.argmax(p.detach().numpy())
  y_true_class = np.argmax(item[1].numpy())
  print(f"The predicted disease is: {dnames[y_pred_class]}, the answer is: {dnames[y_true_class]}")

predict(model, test_ds[2])

predict(model, test_ds[3])

predict(model, test_ds[8])

